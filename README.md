# LLM From Scratch

A PyTorch implementation of a GPT-style Large Language Model built from the ground up.

## Project Overview

This project implements a transformer-based language model similar to GPT, built entirely from scratch to understand the inner workings of modern LLMs.

## Features

- Custom tokenizer implementation
- Transformer architecture with multi-head attention
- Position embeddings
- Training pipeline with GPU acceleration (MPS/CUDA)
- Text generation with various sampling strategies

## Project Structure

```
llm-from-scratch/
├── src/              # Source code
├── data/             # Training data
├── models/           # Saved model checkpoints
├── notebooks/        # Jupyter notebooks for experimentation
├── tests/            # Unit tests
└── requirements.txt  # Python dependencies
```

## Setup

1. Create and activate virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

Coming soon...

## Architecture

Coming soon...

## Training

Coming soon...

## Author

Built as a learning project to understand LLM internals.

## License

MIT
